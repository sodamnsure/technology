## <center><font color=#5C4033>Shuffle过程及优化</font></center>

### <font color=#FF7F00>Shuffle过程</font>
1. `mapper()`方法做业务逻辑处理，然后将数据传到分区方法中，给数据标记好分区，将数据发送到环形缓冲区。
2. 环形缓冲区默认`100M`，达到`80%`的阈值就进行溢写操作。
3. 溢写之前会进行排序，排序的规则是字典序排序，排序的手段是快排。
4. 溢写会产生很多溢写文件，溢写文件默认达到10个就会合并，合并时采用的算法是归并排序。
5. 也可以进行`combiner`局部聚合的操作，前提是局部聚合的结果不会对最终的结果产生影响。
6. 等到所有的`maptask`运行完毕，会启动一定数量的`reducetask`并告知`reducetask`读取数据的范围(也就是分区)。
7. `reducetask`发起拉取线程，去`map`端拉取数据，拉过来的数据会先存储到内存中，内存放不下就放到磁盘，等到数据拉取完毕之后会再进行一个归并排序。
8. 然后再对数据进行分组，以组为单位将数据发送到`reducer()`方法中。


### <font color=#FF7F00>优化</font>

#### <font color=#FAA400>Map阶段</font>
1. 增加环形缓冲区大小，由`100M`扩大到`200M`;
2. 增大环形缓冲区溢写的比例，由`80%`扩大到`90%`；
3. 减少对溢写文件的`merger`次数。
4. 不影响业务的前提下，采用`combiner`提前合并，减少`I/O`;

#### <font color=#FAA400>Reduce阶段</font>
1. 合理设置Map和Reduce数目：两个都不能设置太少，也不能设置太多。太少，会导致`Task`等待，延长处理时间；太多，会导致`map、reduce`任务竞争资源，造成处理超时等错误；
2. 设置`Map、Reduce`共存：调整`slowstart.completedmaps`参数，使`Map`运行到一定程度后，`Reduce`也开始运行，减少`Reduce`的等待时间；
3. 增加每个`Reduce`去`Map`中拿数据的并行度；
4. 集群性能可以的话，增大`Reduce`端存储数据内存的大小；

#### <font color=#FAA400>IO传输</font>
采用数据压缩的方式，减少网络IO的时间。